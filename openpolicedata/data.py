import os.path as path
import pandas as pd
from datetime import datetime

if __name__ == '__main__':
    import data_loaders
    import _datasets
else:
    from . import data_loaders
    from . import _datasets

class Table:
    """
    A class that contains a DataFrame for a dataset along with meta information

    Attributes
    ----------
    details : pandas Series
        Series containing information about the dataset
    state : str
        Name of state where jurisdictions in table are
    source_name : str
        Name of source
    jurisdiction : str
        Name of jurisdiction
    table_type : TableTypes enum
        Type of data contained in table
    year : int, list, MULTI
        Indicates years contained in table
    description : str
        Description of data source
    url : str
        URL where table was accessed from
    table : pandas of geopandasDataFrame
        Data accessed from source

    Methods
    -------
    to_csv(outputDir=None, filename=None)
        Convert table to CSV file
    get_csv_filename()
        Get default name of CSV file
    """

    details = None
    state = None
    source_name = None
    jurisdiction = None
    table_type = None
    year = None
    description = None
    url = None

    # Data
    table = None

    # From source
    _data_type = None
    _dataset_id = None
    _date_field = None
    _jurisdiction_field = None

    def __init__(self, source, table=None, year_filter=None, jurisdiction_filter=None):
        '''Construct Table object
        This is intended to be generated by the Source.load_from_url and Source.load_from_csv classes

        Parameters
        ----------
        source : pandas or geopandas Series
            Series containing information on the source
        table : pandas or geopandas 
            Name of state where jurisdictions in table are
        '''
        if not isinstance(source, pd.core.frame.DataFrame) and \
            not isinstance(source, pd.core.series.Series):
            raise TypeError("data must be an ID, DataFrame or Series")
        elif isinstance(source, pd.core.frame.DataFrame):
            if len(source) == 0:
                raise LookupError("DataFrame is empty")
            elif len(source) > 1:
                raise LookupError("DataFrame has more than 1 row")

            source = source.iloc[0]

        self.details = source
        self.table = table

        self.state = source["State"]
        self.source_name = source["SourceName"]

        if jurisdiction_filter != None:
            self.jurisdiction = jurisdiction_filter
        else:
            self.jurisdiction = source["Jurisdiction"]

        self.table_type = _datasets.TableTypes(source["TableType"])  # Convert to Enum

        if year_filter != None:
            self.year = year_filter
        else:
            self.year = source["Year"]

        self.description = source["Description"]
        self.url = source["URL"]
        self._data_type = _datasets.DataTypes(source["DataType"])  # Convert to Enum

        if not pd.isnull(source["dataset_id"]):
            self._dataset_id = source["dataset_id"]

        if not pd.isnull(source["date_field"]):
            self._date_field = source["date_field"]
        
        if not pd.isnull(source["jurisdiction_field"]):
            self._jurisdiction_field = source["jurisdiction_field"]


    def to_csv(self, outputDir=None, filename=None):
        '''Export table to CSV file. Use default filename for data that will
        be reloaded as an openpolicedata Table object

        Parameters
        ----------
        outputDir - str
            (Optional) Output directory. Default: current directory
        filename - str
            (Optional) Filename. Default: Result of get_csv_filename()
        '''
        if filename == None:
            filename = self.get_csv_filename()
        if outputDir != None:
            filename = path.join(outputDir, filename)
        if not isinstance(self.table, pd.core.frame.DataFrame):
            raise ValueError("There is no table to save to CSV")

        self.table.to_csv(filename, index=False)


    def get_csv_filename(self):
        '''Generate default filename based on table parameters

        Returns
        -------
        str
            Filename
        '''
        return get_csv_filename(self.state, self.source_name, self.jurisdiction, self.table_type, self.year)


class Source:
    """
    Class for exploring a data source and loading its data

    ...

    Attributes
    ----------
    datasets : pandas or geopandas DataFrame
        Contains information on datasets available from the source

    Methods
    -------
    get_tables_types()
        Get types of data availble from the source
    get_years()
        Get years available for 1 or more datasets
    get_jurisdictions()
        Get jurisdictions available for 1 or more datasets
    load_from_url()
        Load data from URL
    load_from_csv()
        Load data from a previously saved CSV file
    """

    datasets = None
    __limit = None

    def __init__(self, source_name, state=None):
        '''Constructor for Source class

        Parameters
        ----------
        source_name - str
            Source name from datasets table
        state - str
            (Optional) Name of state. Only necessary if source_name is not unique.

        Returns
        -------
        Source object
        '''
        self.datasets = _datasets.datasets_query(source_name=source_name, state=state)

        # Ensure that all sources are from the same state
        if len(self.datasets) == 0:
            raise ValueError("No Sources Found")
        elif self.datasets["State"].nunique() > 1:
            raise ValueError("Not all sources are from the same state")


    def get_tables_types(self):
        '''Get types of data availble from the source

        Returns
        -------
        list
            List containing types of data available from source
        '''
        return list(self.datasets["TableType"].unique())


    def get_years(self, table_type=None):
        '''Get years available for 1 or more datasets

        Parameters
        ----------
        table_type - str or TableTypes enum
            (Optional) If set, only returns years for requested table type

        Returns
        -------
        list
            List of years available for 1 or more datasets
        '''
        if isinstance(table_type, _datasets.TableTypes):
            table_type = table_type.value

        df = self.datasets
        if table_type != None:
            df = self.datasets[self.datasets["TableType"]==table_type]

        if len(df) == 1 and df.iloc[0]["Year"] == _datasets.MULTI:
            df = df.iloc[0]

            data_type = _datasets.DataTypes(df["DataType"])
            url = df["URL"]
            if not pd.isnull(df["date_field"]):
                date_field = df["date_field"]
            else:
                raise ValueError("No date_field is provided to identify the years")
            
            if data_type == _datasets.DataTypes.CSV:
                raise NotImplementedError("This needs to be tested before use")
                if force_read:                    
                    table = pd.read_csv(url, parse_dates=True)
                    years = table[date_field].dt.year
                    years = years.unique()
                else:
                    raise ValueError("Getting the year of a CSV files requires reading in the whole file. " +
                                    "Loading in the table may be a better option. If getYears is still desired " +
                                    " for this case, use forceRead=True")
            elif data_type == _datasets.DataTypes.GeoJSON:
                raise NotImplementedError("This needs to be tested before use")
                if force_read:
                    table = data_loaders.load_geojson(url)
                    years = table[date_field].dt.year
                    years = list(years.unique())
                else:
                    raise ValueError("Getting the year of a GeoJSON files requires reading in the whole file. " +
                                    "Loading in the table may be a better option. If getYears is still desired " +
                                    " for this case, use forceRead=True")
                    
            elif data_type == _datasets.DataTypes.ArcGIS:
                years = data_loaders.get_years_argis(url, date_field)
            elif data_type == _datasets.DataTypes.SOCRATA:
                years = data_loaders.get_years_socrata(url, df["dataset_id"], date_field)
            else:
                raise ValueError(f"Unknown data type: {data_type}")
        else:
            years = list(df["Year"].unique())
            
        years.sort()
        return years


    def get_jurisdictions(self, table_type=None, year=None, partial_name=None):
        '''Get jurisdictions available for 1 or more datasets

        Parameters
        ----------
        table_type - str or TableTypes enum
            (Optional) If set, only returns jurisdictions for requested table type
        year - int or the string "MULTI" or "N/A"
            (Optional)  If set, only returns jurisdictions for requested year
        table_type - str or TableTypes enum
            (Optional) If set, only returns jurisdictions for requested table type
        partial_name - str
            (Optional)  If set, only returns jurisdictions containing the substring
            partial_name for datasets that contain multiple jurisdictions

        Returns
        -------
        list
            List of years available for 1 or more datasets
        '''

        if isinstance(table_type, _datasets.TableTypes):
            table_type = table_type.value

        src = self.datasets
        if table_type != None:
            src = self.datasets[self.datasets["TableType"]==table_type]

        if year != None:
            src = src[src["Year"] == year]

        if len(src) == 1:
            src = src.iloc[0]
        else:
            raise ValueError("table_type and year inputs must filter for a single source")            

        # If year is multi, need to use self._jurisdictionField to query URL
        # Otherwise return self.jurisdiction
        if src["Jurisdiction"] == _datasets.MULTI:
            data_type = _datasets.DataTypes(src["DataType"])
            if data_type == _datasets.DataTypes.CSV:
                raise NotImplementedError(f"Unable to get jurisdictions for {data_type}")
            elif data_type == _datasets.DataTypes.GeoJSON:
                raise NotImplementedError(f"Unable to get jurisdictions for {data_type}")
            elif data_type == _datasets.DataTypes.ArcGIS:
                raise NotImplementedError(f"Unable to get jurisdictions for {data_type}")
            elif data_type == _datasets.DataTypes.SOCRATA:
                if partial_name is not None:
                    opt_filter = "agency_name LIKE '%" + partial_name + "%'"
                else:
                    opt_filter = None

                select = "DISTINCT " + src["jurisdiction_field"]
                if year == _datasets.MULTI:
                    year = None
                jurisdictionSet = data_loaders.load_socrata(src["URL"], src["dataset_id"], 
                    date_field=src["date_field"], year=year, opt_filter=opt_filter, select=select, output_type="set")
                return list(jurisdictionSet)
            else:
                raise ValueError(f"Unknown data type: {data_type}")
        else:
            return [src["Jurisdiction"]]
        

    def load_from_url(self, year, table_type=None, jurisdiction_filter=None):
        '''Load data from URL

        Parameters
        ----------
        year - int or length 2 list or the string "MULTI" or "N/A"
            Used to identify the requested dataset if equal to its year value
            Otherwise, for datasets containing multiple years, this filters 
            the return data for a specific year (int input) or a range of years
            [X,Y] to return data for years X to Y
        table_type - str or TableTypes enum
            (Optional) If set, requested dataset will be of this type
        jurisdiction_filter - str
            (Optional) If set, for datasets containing multiple jurisdictions, data will
            only be returned for this jurisdiction

        Returns
        -------
        Table
            Table object containing the requested data
        '''

        return self.__load(year, table_type, jurisdiction_filter, True)

    def __load(self, year, table_type, jurisdiction_filter, load_table):
        if isinstance(table_type, _datasets.TableTypes):
            table_type = table_type.value

        src = self.datasets
        if table_type != None:
            src = src[self.datasets["TableType"] == table_type]

        if isinstance(year, list):
            matchingYears = src["Year"] == year[0]
            for y in year[1:]:
                matchingYears = matchingYears | (src["Year"] == y)
        else:
            matchingYears = src["Year"] == year

        filter_by_year = not matchingYears.any()
        if not filter_by_year:
            # Use source for this specific year if available
            src = src[matchingYears]
        else:
            # If there are not any years corresponding to this year, check for a table
            # containing multiple years
            src = src.query("Year == '" + _datasets.MULTI + "'")

        if isinstance(src, pd.core.frame.DataFrame):
            if len(src) == 0:
                raise ValueError(f"There are no sources matching tableType {table_type} and year {year}")
            elif len(src) > 1:
                raise ValueError(f"There is more than one source matching tableType {table_type} and year {year}")
            else:
                src = src.iloc[0]

        # Load data from URL. For year or jurisdiction equal to multi, filtering can be done
        data_type = _datasets.DataTypes(src["DataType"])
        url = src["URL"]

        if filter_by_year:
            year_filter = year
        else:
            year_filter = None

        if not pd.isnull(src["dataset_id"]):
            dataset_id = src["dataset_id"]

        table_year = None
        if not pd.isnull(src["date_field"]):
            date_field = src["date_field"]
            if year_filter != None:
                table_year = year_filter
        else:
            date_field = None
        
        table_jurisdiction = None
        if not pd.isnull(src["jurisdiction_field"]):
            jurisdiction_field = src["jurisdiction_field"]
            if jurisdiction_filter != None and data_type != _datasets.DataTypes.ArcGIS:
                table_jurisdiction = jurisdiction_filter
        else:
            jurisdiction_field = None
        
        #It is assumed that each data loader method will return data with the proper data type so date type etc...
        if load_table:
            if data_type == _datasets.DataTypes.CSV:
                table = data_loaders.load_csv(url, date_field=date_field, year_filter=year_filter, 
                    jurisdiction_field=jurisdiction_field, jurisdiction_filter=jurisdiction_filter, limit=self.__limit)
            elif data_type == _datasets.DataTypes.GeoJSON:
                table = data_loaders.load_geojson(url, date_field=date_field, year_filter=year_filter, 
                    jurisdiction_field=jurisdiction_field, jurisdiction_filter=jurisdiction_filter)
            elif data_type == _datasets.DataTypes.ArcGIS:
                table = data_loaders.load_arcgis(url, date_field, year_filter, limit=self.__limit)
            elif data_type == _datasets.DataTypes.SOCRATA:
                opt_filter = None
                if jurisdiction_filter != None and jurisdiction_field != None:
                    opt_filter = jurisdiction_field + " = '" + jurisdiction_filter + "'"

                table = data_loaders.load_socrata(url, dataset_id, date_field=date_field, year=year_filter, opt_filter=opt_filter, 
                    limit=self.__limit)
            else:
                raise ValueError(f"Unknown data type: {data_type}")

            table = _check_date(table, date_field)                        
        else:
            table = None

        return Table(src, table, year_filter=table_year, jurisdiction_filter=table_jurisdiction)


    def load_from_csv(self, year, outputDir=None, table_type=None, jurisdiction_filter=None):
        '''Load data from previously saved CSV file
        
        Parameters
        ----------
        year - int or length 2 list or the string "MULTI" or "N/A"
            Used to identify the requested dataset if equal to its year value
            Otherwise, for datasets containing multiple years, this filters 
            the return data for a specific year (int input) or a range of years
            [X,Y] to return data for years X to Y
        outputDir - str
            (Optional) Directory where CSV file is stored
        table_type - str or TableTypes enum
            (Optional) If set, requested dataset will be of this type
        jurisdiction_filter - str
            (Optional) If set, for datasets containing multiple jurisdictions, data will
            only be returned for this jurisdiction

        Returns
        -------
        Table
            Table object containing the requested data
        '''

        table = self.__load(year, table_type, jurisdiction_filter, False)

        filename = table.get_csv_filename()
        if outputDir != None:
            filename = path.join(outputDir, filename)            

        table.table = pd.read_csv(filename, parse_dates=True)
        table.table = _check_date(table.table, table._date_field)  

        return table


def _check_date(table, date_field):
    if date_field != None and len(table)>0:
        dts = table[date_field]
        dts = dts[dts.notnull()]
        if len(dts) > 0:
            one_date = dts.iloc[0]            
            if type(one_date) == str:
                table = table.astype({date_field: 'datetime64[ns]'})
                dts = table[date_field]
                dts = dts[dts.notnull()]
                one_date = dts.iloc[0]
            elif date_field.lower() == "year":
                try:
                    float(one_date)
                except:
                    raise

                table[date_field] = table[date_field].apply(lambda x: datetime(x,1,1))
                dts = table[date_field]
                dts = dts[dts.notnull()]
                one_date = dts.iloc[0]
                
            # Replace bad dates with NaT
            table[date_field].replace(datetime.strptime('1900-01-01 00:00:00', '%Y-%m-%d %H:%M:%S'), pd.NaT, inplace=True)
            dts = table[date_field]
            dts = dts[dts.notnull()]

            if len(dts) > 0:
                one_date = dts.iloc[0] 
            if hasattr(one_date, "year"):
                if one_date.year < 1995:
                    raise ValueError("Date is before 1995. There was likely an issue in the date conversion")
                elif one_date.year > datetime.now().year:
                    raise ValueError("Date is after the current year. There was likely an issue in the date conversion")
            else:
                raise TypeError("Unknown data type for date")

    return table


def get_csv_filename(state, source_name, jurisdiction, table_type, year):
    '''Get default CSV filename for the given parameters. Enables reloading of data from CSV.
    
    Parameters
    ----------
    state - str
        Name of state
    source_name - str
        Name of source
    jurisdiction - str
        Name of jurisdiction
    table_type - str or TableTypes enum
        Type of data
    year = int or length 2 list or the string "MULTI" or "N/A"
        Year of data to load, range of years of data to load as a list [X,Y]
        to load years X to Y, or a string to indicate all of multiple year data
        ("MULTI") or a dataset that has no year filtering ("N/A")

    Returns
    -------
    str
        Default CSV filename
    '''
    if isinstance(table_type, _datasets.TableTypes):
        table_type = table_type.value
        
    filename = f"{state}_{source_name}"
    if source_name != jurisdiction:
        filename += f"_{jurisdiction}"
    filename += f"_{table_type}"
    if isinstance(year, list):
        filename += f"_{year[0]}_{year[-1]}"
    else:
        filename += f"_{year}"

    # Clean up filename
    filename = filename.replace(",", "_").replace(" ", "_").replace("__", "_").replace("/", "_")

    filename += ".csv"

    return filename

if __name__ == '__main__':
    src = Source("Baltimore")

    t = src.load_from_url(year=2017, table_type=_datasets.TableTypes.CALLS_FOR_SERVICE)

    print("data main function complete")
